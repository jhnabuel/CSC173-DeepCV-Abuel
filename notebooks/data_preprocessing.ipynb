{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d17bf8da",
   "metadata": {},
   "source": [
    "# Deep Learning for Tuberculosis Detection and Classification: A YOLO-v8 + Convolutional Block Attention Module Approach\n",
    "\n",
    "By: John Abuel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c6cad5",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fe59ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c9fad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "  \n",
    "    TRAIN_JSON = \"../dataset/raw_data/TBX11K/annotations/json/all_train.json\"\n",
    "    VAL_JSON   = \"../dataset/raw_data/TBX11K/annotations/json/all_val.json\"\n",
    "    \n",
    "    # Optional: For final deployment only (Combines train+val)\n",
    "    # TRAINVAL_JSON = \"../dataset/raw_data/TBX11K/annotations/json/all_trainval.json\"\n",
    "\n",
    "    # 2. IMAGE ROOT\n",
    "    # Contains 'tb', 'health', 'sick' AND 'extra' folders\n",
    "    RAW_IMAGES = \"../dataset/raw_data/TBX11K/imgs\" \n",
    "    \n",
    "    # 3. OUTPUT PATHS\n",
    "    OUTPUT_BASE = \"../dataset/yolo_official\"\n",
    "    \n",
    "    # Parameters\n",
    "    MIN_BOX_AREA = 50       \n",
    "    TARGET_SIZE = (512, 512)\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898f158",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed0ba6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(json_path):\n",
    "    print(f\"Loading annotations from {json_path}...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    images = {}\n",
    "    for img in data['images']:\n",
    "        images[img['id']] = img\n",
    "        images[img['id']]['annotations'] = []\n",
    "        \n",
    "    for ann in data['annotations']:\n",
    "        if ann['image_id'] in images:\n",
    "            # Map Category Name for convenience\n",
    "            cat_id = ann['category_id']\n",
    "            # TBX11K IDs: 1=Active, 2=Latent, 3=Uncertain\n",
    "            if cat_id == 1: name = 'ActiveTuberculosis'\n",
    "            elif cat_id == 2: name = 'ObsoletePulmonaryTuberculosis'\n",
    "            else: name = 'Uncertain'\n",
    "            \n",
    "            ann['category_name'] = name\n",
    "            images[ann['image_id']]['annotations'].append(ann)\n",
    "            \n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0043d11",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a85fb9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(images_data, config):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA CLEANING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    cleaned_data = {}\n",
    "    removed_count = 0\n",
    "    \n",
    "    for img_id, img_info in images_data.items():\n",
    "        src_path = os.path.join(config.RAW_IMAGES, img_info['file_name'])\n",
    "        \n",
    "        # 1. Check if file exists\n",
    "        if not os.path.exists(src_path):\n",
    "            removed_count += 1\n",
    "            continue\n",
    "\n",
    "        # 2. Filter Invalid Boxes\n",
    "        original_ann_count = len(img_info['annotations'])\n",
    "        valid_annotations = []\n",
    "        \n",
    "        for ann in img_info['annotations']:\n",
    "            bbox = ann['bbox'] # [x, y, w, h]\n",
    "            # Ensure box has positive area and meets threshold\n",
    "            if bbox[2] > 0 and bbox[3] > 0:\n",
    "                if (bbox[2] * bbox[3]) >= config.MIN_BOX_AREA:\n",
    "                    valid_annotations.append(ann)\n",
    "        \n",
    "        # 3. LOGIC FIX:\n",
    "        # If image HAD boxes (was TB), but we filtered them all out (too small),\n",
    "        # DROP the image. Do not label a TB patient as \"Healthy\" just because boxes were small.\n",
    "        if original_ann_count > 0 and len(valid_annotations) == 0:\n",
    "            removed_count += 1\n",
    "            continue \n",
    "\n",
    "        # If it was Healthy (0 boxes) or has valid boxes, keep it.\n",
    "        img_info['annotations'] = valid_annotations\n",
    "        cleaned_data[img_id] = img_info\n",
    "\n",
    "    print(f\"✓ Original dataset: {len(images_data)} images\")\n",
    "    print(f\"✓ Cleaned dataset: {len(cleaned_data)} images\")\n",
    "    print(f\"✓ Removed: {removed_count} images (missing files or invalid boxes)\")\n",
    "    \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99932d",
   "metadata": {},
   "source": [
    "## Data Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99cf4555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_id(category_name):\n",
    "    \"\"\"Map categories to YOLO IDs: Active=0, Latent=1\"\"\"\n",
    "    name = str(category_name).lower()\n",
    "    if 'active' in name: return 0\n",
    "    elif 'obsolete' in name or 'latent' in name: return 1\n",
    "    return -1 # Skip\n",
    "\n",
    "def convert_to_yolo_format(bbox, img_w, img_h):\n",
    "    # COCO [x, y, w, h] -> YOLO [cx, cy, w, h] normalized\n",
    "    x, y, w, h = bbox\n",
    "    cx = (x + w/2) / img_w\n",
    "    cy = (y + h/2) / img_h\n",
    "    nw = w / img_w\n",
    "    nh = h / img_h\n",
    "    return [max(0, min(1, val)) for val in [cx, cy, nw, nh]]\n",
    "\n",
    "def process_json_file(json_path, split_name, config):\n",
    "    \"\"\"\n",
    "    Loads an official JSON file, cleans it, and writes images/labels \n",
    "    to the specific split folder (train/val).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"⚠️  Skipping {split_name}: File not found at {json_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"PROCESSING SPLIT: {split_name.upper()}\")\n",
    "    print(f\"Source: {os.path.basename(json_path)}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1. Setup Output Directories\n",
    "    img_dir = os.path.join(config.OUTPUT_BASE, \"images\", split_name)\n",
    "    lbl_dir = os.path.join(config.OUTPUT_BASE, \"labels\", split_name)\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "    os.makedirs(lbl_dir, exist_ok=True)\n",
    "\n",
    "    # 2. Load Data\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Map Images\n",
    "    images_lookup = {img['id']: img for img in data['images']}\n",
    "    \n",
    "    # Map Annotations\n",
    "    anns_lookup = {}\n",
    "    for ann in data['annotations']:\n",
    "        img_id = ann['image_id']\n",
    "        if img_id not in anns_lookup: anns_lookup[img_id] = []\n",
    "        \n",
    "        # Inject Category Name if missing\n",
    "        if 'category_name' not in ann:\n",
    "            cid = ann['category_id']\n",
    "            if cid == 1: ann['category_name'] = 'ActiveTuberculosis'\n",
    "            elif cid == 2: ann['category_name'] = 'ObsoletePulmonaryTuberculosis'\n",
    "            else: ann['category_name'] = 'Unknown'\n",
    "            \n",
    "        anns_lookup[img_id].append(ann)\n",
    "\n",
    "    # 3. Process Images\n",
    "    processed_count = 0\n",
    "    missing_files = 0\n",
    "    \n",
    "    for img_id, img_info in tqdm(images_lookup.items(), desc=f\"Converting {split_name}\"):\n",
    "        # Construct path (handles 'extra/Shenzhen/...' automatically)\n",
    "        src_path = os.path.join(config.RAW_IMAGES, img_info['file_name'])\n",
    "        \n",
    "        # Validation: Check file existence\n",
    "        if not os.path.exists(src_path):\n",
    "            missing_files += 1\n",
    "            continue\n",
    "\n",
    "        # Flatten Filename for YOLO structure\n",
    "        flat_name = os.path.basename(img_info['file_name'])\n",
    "        dst_img_path = os.path.join(img_dir, flat_name)\n",
    "        dst_lbl_path = os.path.join(lbl_dir, os.path.splitext(flat_name)[0] + \".txt\")\n",
    "\n",
    "        # Read & Resize Image\n",
    "        img = cv2.imread(src_path)\n",
    "        if img is None: \n",
    "            missing_files += 1\n",
    "            continue\n",
    "            \n",
    "        h_orig, w_orig = img.shape[:2]\n",
    "        img_resized = cv2.resize(img, config.TARGET_SIZE)\n",
    "        cv2.imwrite(dst_img_path, img_resized)\n",
    "\n",
    "        # Write Labels\n",
    "        valid_box_count = 0\n",
    "        with open(dst_lbl_path, 'w') as f:\n",
    "            if img_id in anns_lookup:\n",
    "                for ann in anns_lookup[img_id]:\n",
    "                    # Filter: Tiny Boxes (Noise)\n",
    "                    bbox = ann['bbox']\n",
    "                    if bbox[2] * bbox[3] < config.MIN_BOX_AREA: \n",
    "                        continue\n",
    "                    \n",
    "                    # Filter: Class Mapping\n",
    "                    cls_id = get_class_id(ann['category_name'])\n",
    "                    if cls_id == -1: \n",
    "                        continue \n",
    "                    \n",
    "                    # Write\n",
    "                    yolo_box = convert_to_yolo_format(bbox, w_orig, h_orig)\n",
    "                    f.write(f\"{cls_id} {' '.join(map(str, yolo_box))}\\n\")\n",
    "                    valid_box_count += 1\n",
    "        \n",
    "        processed_count += 1\n",
    "\n",
    "    print(f\"✓ Finished {split_name}: {processed_count} images processed.\")\n",
    "    if missing_files > 0:\n",
    "        print(f\"  Warning: {missing_files} images were missing from disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ea0f8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING SPLIT: TRAIN\n",
      "Source: all_train.json\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train: 100%|██████████| 6888/6888 [01:31<00:00, 75.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Finished train: 6888 images processed.\n",
      "\n",
      "============================================================\n",
      "PROCESSING SPLIT: VAL\n",
      "Source: all_val.json\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting val: 100%|██████████| 2088/2088 [00:28<00:00, 73.83it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Finished val: 2088 images processed.\n",
      "\n",
      " SUCCESS! Dataset is ready at: ../dataset/yolo_official\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. Process Official Train Set\n",
    "    process_json_file(config.TRAIN_JSON, \"train\", config)\n",
    "    \n",
    "    # 2. Process Official Val Set\n",
    "    process_json_file(config.VAL_JSON, \"val\", config)\n",
    "    \n",
    "    # NOTE: We do NOT process 'all_test.json' because it has no labels. \n",
    "    # YOLO cannot train on images without .txt files.\n",
    "    \n",
    "    print(\"\\n SUCCESS! Dataset is ready at:\", config.OUTPUT_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa6381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
